{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f74a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/i574473/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630e0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "  \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)  \n",
    "    separator = ' '    \n",
    "    return separator.join(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffc3f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['little', 'boy', 'named', 'andy', 'loves', 'room', 'playing', 'toys', 'especially', 'doll', 'named', '``', 'woody', '``', 'but', 'toys', 'andy', 'come', 'life', 'woody', 'believes', 'life', 'toy', 'good', 'however', 'must', 'worry', 'andy', \"'s\", 'family', 'moving', 'woody', 'know', 'andy', \"'s\", 'birthday', 'party', 'woody', 'realize', 'andy', \"'s\", 'mother', 'gave', 'action', 'figure', 'known', 'buzz', 'lightyear', 'believe', 'toy', 'quickly', 'becomes', 'andy', \"'s\", 'new', 'favorite', 'toy', 'woody', 'consumed', 'jealousy', 'tries', 'get', 'rid', 'buzz', 'then', 'woody', 'buzz', 'lost', 'they', 'must', 'find', 'way', 'get', 'back', 'andy', 'moves', 'without', 'pass', 'ruthless', 'toy', 'killer', 'sid', 'phillips'], [5]>\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "# Function for tokenizing\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    text_filtered = remove_english_stopwords(text)\n",
    "    for sent in nltk.sent_tokenize(text_filtered):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train_documents = []\n",
    "test_documents = []\n",
    "i = 0\n",
    "\n",
    "tags_index = {'sci-fi': 1 , 'action': 2, 'comedy': 3, 'fantasy': 4, 'animation': 5, 'romance': 6}\n",
    "#Reading the file\n",
    "FILEPATH = 'movies.csv'\n",
    "with open(FILEPATH, 'r') as csvfile:\n",
    "    moviereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in moviereader:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        if i <= 2000:\n",
    "            train_documents.append(TaggedDocument(words=tokenize_text(row[2]), tags=[tags_index.get(row[3], 8)] ))\n",
    "        else:\n",
    "            test_documents.append(TaggedDocument(words=tokenize_text(row[2]), tags=[tags_index.get(row[3], 8)]))\n",
    "print(train_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e41e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 1999/1999 [00:00<00:00, 2467455.47it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_documents)])\n",
    "train_documents  = utils.shuffle(train_documents)\n",
    "model_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "#    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "model_dbow.save('./movieModel.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abfc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "y_train, X_train = vector_for_learning(model_dbow, train_documents)\n",
    "y_test, X_test = vector_for_learning(model_dbow, test_documents)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "# X_test_scaled = min_max_scaler.transform(X_test)\n",
    "\n",
    "# min_max_scaler = preprocessing.MaxAbsScaler()\n",
    "# X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "# X_test_scaled = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97bada0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "model = LogisticRegression(n_jobs=1, C=1e5, max_iter=2000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e26481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for movie plots0.43207126948775054\n",
      "Testing F1 score for movie plots: 0.4283289240078341\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy for movie plots%s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score for movie plots: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09a199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
